{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask_detect.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jinchJPujXRG"
      },
      "source": [
        "**Import all the libraries and modules required.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HzxTFTReWCq"
      },
      "source": [
        "from keras.optimizers import RMSprop\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "import cv2\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\r\n",
        "from keras.models import Model, load_model\r\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.utils import shuffle\r\n",
        "import imutils\r\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5W5FQX7jckf"
      },
      "source": [
        "**Build the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ipz8rq_ezuR"
      },
      "source": [
        "model = Sequential([\r\n",
        "    Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\r\n",
        "    MaxPooling2D(2,2),\r\n",
        "    \r\n",
        "    Conv2D(100, (3,3), activation='relu'),\r\n",
        "    MaxPooling2D(2,2),\r\n",
        "    \r\n",
        "    Flatten(),\r\n",
        "    Dropout(0.5),\r\n",
        "    Dense(50, activation='relu'),\r\n",
        "    Dense(2, activation='softmax')\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjPXlmvOjnPX"
      },
      "source": [
        "**Image Data Generation/Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9JNPovAjD94",
        "outputId": "ffe609b6-a076-48b7-bc8b-cc674c4de73a"
      },
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/Dataset/train/train\"\r\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255,\r\n",
        "                                   rotation_range=40,\r\n",
        "                                   width_shift_range=0.2,\r\n",
        "                                   height_shift_range=0.2,\r\n",
        "                                   shear_range=0.2,\r\n",
        "                                   zoom_range=0.2,\r\n",
        "                                   horizontal_flip=True,\r\n",
        "                                   fill_mode='nearest')\r\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \r\n",
        "                                                    batch_size=10, \r\n",
        "                                                    target_size=(150, 150))\r\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/Dataset/test/test\"\r\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255)\r\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \r\n",
        "                                                         batch_size=10, \r\n",
        "                                                         target_size=(150, 150))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1327 images belonging to 2 classes.\n",
            "Found 194 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSGWqt-yr_LR"
      },
      "source": [
        "**Saving best model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etPzV7uvr07r"
      },
      "source": [
        "checkpoint = ModelCheckpoint('model2-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlbip5XNsLc3"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYfBmVwHsP7W",
        "outputId": "ac34b98a-bd37-4dad-cd25-73be3dabc8e9"
      },
      "source": [
        "history = model.fit_generator(train_generator,\r\n",
        "                              epochs=100,\r\n",
        "                              validation_data=validation_generator,\r\n",
        "                              callbacks=[checkpoint])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "133/133 [==============================] - 737s 6s/step - loss: 0.5535 - acc: 0.7280 - val_loss: 0.1775 - val_acc: 0.9433\n",
            "INFO:tensorflow:Assets written to: model2-001.model/assets\n",
            "Epoch 2/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.3217 - acc: 0.8734 - val_loss: 0.2490 - val_acc: 0.9742\n",
            "Epoch 3/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.2971 - acc: 0.8855 - val_loss: 0.1026 - val_acc: 0.9588\n",
            "INFO:tensorflow:Assets written to: model2-003.model/assets\n",
            "Epoch 4/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.2271 - acc: 0.9171 - val_loss: 0.0696 - val_acc: 0.9845\n",
            "INFO:tensorflow:Assets written to: model2-004.model/assets\n",
            "Epoch 5/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.2433 - acc: 0.8975 - val_loss: 0.1894 - val_acc: 0.8969\n",
            "Epoch 6/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.2357 - acc: 0.9179 - val_loss: 0.0835 - val_acc: 0.9691\n",
            "Epoch 7/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1908 - acc: 0.9299 - val_loss: 0.0579 - val_acc: 0.9845\n",
            "INFO:tensorflow:Assets written to: model2-007.model/assets\n",
            "Epoch 8/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.1758 - acc: 0.9292 - val_loss: 0.0406 - val_acc: 0.9897\n",
            "INFO:tensorflow:Assets written to: model2-008.model/assets\n",
            "Epoch 9/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1813 - acc: 0.9344 - val_loss: 0.0442 - val_acc: 0.9845\n",
            "Epoch 10/100\n",
            "133/133 [==============================] - 19s 143ms/step - loss: 0.1935 - acc: 0.9314 - val_loss: 0.0392 - val_acc: 0.9897\n",
            "INFO:tensorflow:Assets written to: model2-010.model/assets\n",
            "Epoch 11/100\n",
            "133/133 [==============================] - 19s 142ms/step - loss: 0.1577 - acc: 0.9427 - val_loss: 0.0244 - val_acc: 1.0000\n",
            "INFO:tensorflow:Assets written to: model2-011.model/assets\n",
            "Epoch 12/100\n",
            "133/133 [==============================] - 19s 142ms/step - loss: 0.1714 - acc: 0.9322 - val_loss: 0.1066 - val_acc: 0.9536\n",
            "Epoch 13/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1735 - acc: 0.9344 - val_loss: 0.0491 - val_acc: 0.9845\n",
            "Epoch 14/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.1723 - acc: 0.9367 - val_loss: 0.0346 - val_acc: 0.9897\n",
            "Epoch 15/100\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.1666 - acc: 0.9337 - val_loss: 0.0333 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1518 - acc: 0.9412 - val_loss: 0.1018 - val_acc: 0.9485\n",
            "Epoch 17/100\n",
            "133/133 [==============================] - 19s 142ms/step - loss: 0.1492 - acc: 0.9510 - val_loss: 0.1327 - val_acc: 0.9381\n",
            "Epoch 18/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1511 - acc: 0.9450 - val_loss: 0.0312 - val_acc: 0.9897\n",
            "Epoch 19/100\n",
            "133/133 [==============================] - 19s 143ms/step - loss: 0.1397 - acc: 0.9503 - val_loss: 0.0272 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1410 - acc: 0.9465 - val_loss: 0.0303 - val_acc: 0.9897\n",
            "Epoch 21/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.1515 - acc: 0.9533 - val_loss: 0.1521 - val_acc: 0.9227\n",
            "Epoch 22/100\n",
            "133/133 [==============================] - 19s 143ms/step - loss: 0.1538 - acc: 0.9420 - val_loss: 0.0714 - val_acc: 0.9639\n",
            "Epoch 23/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1704 - acc: 0.9405 - val_loss: 0.0479 - val_acc: 0.9897\n",
            "Epoch 24/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1381 - acc: 0.9578 - val_loss: 0.0688 - val_acc: 0.9588\n",
            "Epoch 25/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1143 - acc: 0.9623 - val_loss: 0.0206 - val_acc: 0.9948\n",
            "INFO:tensorflow:Assets written to: model2-025.model/assets\n",
            "Epoch 26/100\n",
            "133/133 [==============================] - 19s 143ms/step - loss: 0.1491 - acc: 0.9465 - val_loss: 0.0724 - val_acc: 0.9691\n",
            "Epoch 27/100\n",
            "133/133 [==============================] - 19s 143ms/step - loss: 0.1655 - acc: 0.9352 - val_loss: 0.0393 - val_acc: 0.9845\n",
            "Epoch 28/100\n",
            "133/133 [==============================] - 19s 142ms/step - loss: 0.1212 - acc: 0.9518 - val_loss: 0.0264 - val_acc: 0.9948\n",
            "Epoch 29/100\n",
            "133/133 [==============================] - 19s 142ms/step - loss: 0.1411 - acc: 0.9442 - val_loss: 0.0691 - val_acc: 0.9691\n",
            "Epoch 30/100\n",
            "133/133 [==============================] - 19s 141ms/step - loss: 0.1222 - acc: 0.9586 - val_loss: 0.0349 - val_acc: 0.9948\n",
            "Epoch 31/100\n",
            "133/133 [==============================] - 19s 142ms/step - loss: 0.1371 - acc: 0.9525 - val_loss: 0.0344 - val_acc: 0.9897\n",
            "Epoch 32/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1368 - acc: 0.9495 - val_loss: 0.0185 - val_acc: 0.9948\n",
            "INFO:tensorflow:Assets written to: model2-032.model/assets\n",
            "Epoch 33/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.0967 - acc: 0.9661 - val_loss: 0.0355 - val_acc: 0.9948\n",
            "Epoch 34/100\n",
            "133/133 [==============================] - 19s 143ms/step - loss: 0.1121 - acc: 0.9586 - val_loss: 0.1091 - val_acc: 0.9485\n",
            "Epoch 35/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1195 - acc: 0.9570 - val_loss: 0.0516 - val_acc: 0.9742\n",
            "Epoch 36/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.1081 - acc: 0.9623 - val_loss: 0.0259 - val_acc: 0.9948\n",
            "Epoch 37/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1553 - acc: 0.9420 - val_loss: 0.0304 - val_acc: 0.9897\n",
            "Epoch 38/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.1194 - acc: 0.9540 - val_loss: 0.0333 - val_acc: 0.9948\n",
            "Epoch 39/100\n",
            "133/133 [==============================] - 19s 142ms/step - loss: 0.1139 - acc: 0.9601 - val_loss: 0.0254 - val_acc: 0.9897\n",
            "Epoch 40/100\n",
            "133/133 [==============================] - 19s 142ms/step - loss: 0.0989 - acc: 0.9683 - val_loss: 0.0375 - val_acc: 0.9794\n",
            "Epoch 41/100\n",
            "133/133 [==============================] - 19s 143ms/step - loss: 0.1047 - acc: 0.9593 - val_loss: 0.1059 - val_acc: 0.9433\n",
            "Epoch 42/100\n",
            "133/133 [==============================] - 19s 144ms/step - loss: 0.1091 - acc: 0.9616 - val_loss: 0.0174 - val_acc: 0.9948\n",
            "INFO:tensorflow:Assets written to: model2-042.model/assets\n",
            "Epoch 43/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1505 - acc: 0.9495 - val_loss: 0.0648 - val_acc: 0.9691\n",
            "Epoch 44/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.1189 - acc: 0.9638 - val_loss: 0.0197 - val_acc: 0.9948\n",
            "Epoch 45/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1048 - acc: 0.9586 - val_loss: 0.0290 - val_acc: 0.9897\n",
            "Epoch 46/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1042 - acc: 0.9653 - val_loss: 0.0368 - val_acc: 0.9948\n",
            "Epoch 47/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1008 - acc: 0.9661 - val_loss: 0.0410 - val_acc: 0.9794\n",
            "Epoch 48/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.0872 - acc: 0.9699 - val_loss: 0.0256 - val_acc: 0.9948\n",
            "Epoch 49/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1427 - acc: 0.9555 - val_loss: 0.0434 - val_acc: 0.9845\n",
            "Epoch 50/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.1036 - acc: 0.9623 - val_loss: 0.0400 - val_acc: 0.9794\n",
            "Epoch 51/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0888 - acc: 0.9661 - val_loss: 0.0203 - val_acc: 0.9948\n",
            "Epoch 52/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1001 - acc: 0.9623 - val_loss: 0.0952 - val_acc: 0.9536\n",
            "Epoch 53/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.0989 - acc: 0.9653 - val_loss: 0.0291 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1015 - acc: 0.9631 - val_loss: 0.0175 - val_acc: 0.9948\n",
            "Epoch 55/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1064 - acc: 0.9661 - val_loss: 0.0213 - val_acc: 0.9948\n",
            "Epoch 56/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.0900 - acc: 0.9706 - val_loss: 0.1928 - val_acc: 0.9278\n",
            "Epoch 57/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1120 - acc: 0.9578 - val_loss: 0.2526 - val_acc: 0.8814\n",
            "Epoch 58/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1064 - acc: 0.9555 - val_loss: 0.0169 - val_acc: 0.9948\n",
            "INFO:tensorflow:Assets written to: model2-058.model/assets\n",
            "Epoch 59/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1018 - acc: 0.9638 - val_loss: 0.0256 - val_acc: 0.9897\n",
            "Epoch 60/100\n",
            "133/133 [==============================] - 20s 146ms/step - loss: 0.0800 - acc: 0.9683 - val_loss: 0.0345 - val_acc: 0.9948\n",
            "Epoch 61/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0880 - acc: 0.9691 - val_loss: 0.0193 - val_acc: 0.9948\n",
            "Epoch 62/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1284 - acc: 0.9548 - val_loss: 0.0173 - val_acc: 0.9948\n",
            "Epoch 63/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1058 - acc: 0.9570 - val_loss: 0.0804 - val_acc: 0.9485\n",
            "Epoch 64/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0951 - acc: 0.9691 - val_loss: 0.0153 - val_acc: 0.9948\n",
            "INFO:tensorflow:Assets written to: model2-064.model/assets\n",
            "Epoch 65/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.1115 - acc: 0.9540 - val_loss: 0.0209 - val_acc: 0.9948\n",
            "Epoch 66/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.0918 - acc: 0.9661 - val_loss: 0.0258 - val_acc: 0.9897\n",
            "Epoch 67/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.0966 - acc: 0.9623 - val_loss: 0.0688 - val_acc: 0.9639\n",
            "Epoch 68/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0951 - acc: 0.9661 - val_loss: 0.0563 - val_acc: 0.9639\n",
            "Epoch 69/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.1005 - acc: 0.9540 - val_loss: 0.0440 - val_acc: 0.9845\n",
            "Epoch 70/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.0910 - acc: 0.9631 - val_loss: 0.0801 - val_acc: 0.9639\n",
            "Epoch 71/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0799 - acc: 0.9706 - val_loss: 0.0231 - val_acc: 0.9897\n",
            "Epoch 72/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0844 - acc: 0.9646 - val_loss: 0.0975 - val_acc: 0.9742\n",
            "Epoch 73/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.0857 - acc: 0.9714 - val_loss: 0.0216 - val_acc: 0.9897\n",
            "Epoch 74/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0884 - acc: 0.9676 - val_loss: 0.0159 - val_acc: 0.9948\n",
            "Epoch 75/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0994 - acc: 0.9623 - val_loss: 0.0405 - val_acc: 0.9845\n",
            "Epoch 76/100\n",
            "133/133 [==============================] - 19s 145ms/step - loss: 0.0789 - acc: 0.9721 - val_loss: 0.0228 - val_acc: 0.9948\n",
            "Epoch 77/100\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.0867 - acc: 0.9729 - val_loss: 0.0066 - val_acc: 1.0000\n",
            "INFO:tensorflow:Assets written to: model2-077.model/assets\n",
            "Epoch 78/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.1087 - acc: 0.9631 - val_loss: 0.0418 - val_acc: 0.9897\n",
            "Epoch 79/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.0741 - acc: 0.9797 - val_loss: 0.0155 - val_acc: 0.9948\n",
            "Epoch 80/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0742 - acc: 0.9706 - val_loss: 0.0140 - val_acc: 0.9948\n",
            "Epoch 81/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0852 - acc: 0.9661 - val_loss: 0.0129 - val_acc: 0.9948\n",
            "Epoch 82/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0721 - acc: 0.9766 - val_loss: 0.0102 - val_acc: 0.9948\n",
            "Epoch 83/100\n",
            "133/133 [==============================] - 19s 147ms/step - loss: 0.0637 - acc: 0.9766 - val_loss: 0.0051 - val_acc: 1.0000\n",
            "INFO:tensorflow:Assets written to: model2-083.model/assets\n",
            "Epoch 84/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.0799 - acc: 0.9714 - val_loss: 0.1187 - val_acc: 0.9536\n",
            "Epoch 85/100\n",
            "133/133 [==============================] - 19s 146ms/step - loss: 0.0929 - acc: 0.9608 - val_loss: 0.0500 - val_acc: 0.9742\n",
            "Epoch 86/100\n",
            "133/133 [==============================] - 20s 145ms/step - loss: 0.1042 - acc: 0.9646 - val_loss: 0.0261 - val_acc: 0.9948\n",
            "Epoch 87/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.0678 - acc: 0.9744 - val_loss: 0.0789 - val_acc: 0.9588\n",
            "Epoch 88/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.0898 - acc: 0.9616 - val_loss: 0.0116 - val_acc: 0.9948\n",
            "Epoch 89/100\n",
            "133/133 [==============================] - 20s 147ms/step - loss: 0.0685 - acc: 0.9759 - val_loss: 0.0127 - val_acc: 0.9948\n",
            "Epoch 90/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.0707 - acc: 0.9751 - val_loss: 0.1186 - val_acc: 0.9433\n",
            "Epoch 91/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.1084 - acc: 0.9608 - val_loss: 0.1092 - val_acc: 0.9536\n",
            "Epoch 92/100\n",
            "133/133 [==============================] - 20s 149ms/step - loss: 0.0733 - acc: 0.9751 - val_loss: 0.0145 - val_acc: 0.9948\n",
            "Epoch 93/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.0723 - acc: 0.9751 - val_loss: 0.0278 - val_acc: 0.9948\n",
            "Epoch 94/100\n",
            "133/133 [==============================] - 20s 149ms/step - loss: 0.0794 - acc: 0.9699 - val_loss: 0.0304 - val_acc: 0.9897\n",
            "Epoch 95/100\n",
            "133/133 [==============================] - 20s 149ms/step - loss: 0.0713 - acc: 0.9721 - val_loss: 0.0168 - val_acc: 0.9897\n",
            "Epoch 96/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.0757 - acc: 0.9691 - val_loss: 0.0268 - val_acc: 0.9845\n",
            "Epoch 97/100\n",
            "133/133 [==============================] - 20s 148ms/step - loss: 0.0688 - acc: 0.9751 - val_loss: 0.0095 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "133/133 [==============================] - 20s 149ms/step - loss: 0.0648 - acc: 0.9736 - val_loss: 0.0082 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "133/133 [==============================] - 20s 149ms/step - loss: 0.0673 - acc: 0.9736 - val_loss: 0.0154 - val_acc: 0.9948\n",
            "Epoch 100/100\n",
            "133/133 [==============================] - 20s 149ms/step - loss: 0.0735 - acc: 0.9744 - val_loss: 0.0148 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOdi_7M466v6"
      },
      "source": [
        "**Download the best Model checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM2332fpUOk_"
      },
      "source": [
        "!zip -r /content/BestModel.zip /content/model2-077.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SHPVRG91CjD"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.download('model2-077.model') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP0yHHDSvaiv"
      },
      "source": [
        "**TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvppdBZvZhj"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np\r\n",
        "from keras.models import load_model\r\n",
        "model=load_model(\"/content/model2-010.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAomoXttyoAG"
      },
      "source": [
        "results={0:'without mask',1:'mask'}\r\n",
        "GR_dict={0:(0,0,255),1:(0,255,0)}\r\n",
        "rect_size = 4\r\n",
        "# cap = cv2.VideoCapture(0) \r\n",
        "\r\n",
        "haarcascade = cv2.CascadeClassifier('/content/Haar/haarcascade_frontalface_default.xml')\r\n",
        "while True:\r\n",
        "    (rval, im) = Image(filename)\r\n",
        "    im=cv2.flip(im,1,1) \r\n",
        "    \r\n",
        "    rerect_size = cv2.resize(im, (im.shape[1] // rect_size, im.shape[0] // rect_size))\r\n",
        "    faces = haarcascade.detectMultiScale(rerect_size)\r\n",
        "    for f in faces:\r\n",
        "        (x, y, w, h) = [v * rect_size for v in f] \r\n",
        "        \r\n",
        "        face_img = im[y:y+h, x:x+w]\r\n",
        "        rerect_sized=cv2.resize(face_img,(150,150))\r\n",
        "        normalized=rerect_sized/255.0\r\n",
        "        reshaped=np.reshape(normalized,(1,150,150,3))\r\n",
        "        reshaped = np.vstack([reshaped])\r\n",
        "        result=model.predict(reshaped)\r\n",
        "        \r\n",
        "        label=np.argmax(result,axis=1)[0]\r\n",
        "      \r\n",
        "        cv2.rectangle(im,(x,y),(x+w,y+h),GR_dict[label],2)\r\n",
        "        cv2.rectangle(im,(x,y-40),(x+w,y),GR_dict[label],-1)\r\n",
        "        cv2.putText(im, results[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\r\n",
        "    cv2.imshow('LIVE',   im)\r\n",
        "    key = cv2.waitKey(10)\r\n",
        "    \r\n",
        "    if key == 27: \r\n",
        "        break\r\n",
        "cap.release()\r\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}